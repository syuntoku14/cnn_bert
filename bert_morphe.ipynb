{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from pytorch_pretrained_bert import BertModel, BertTokenizer\n",
    "import string\n",
    "import re\n",
    "import sys\n",
    "import argparse\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# import utils\n",
    "from Interpreter import calculate_regularization, Interpreter\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_word_unfeasible(word):\n",
    "    def is_ascii(word):\n",
    "        return all(ord(c) < 128 for c in word)\n",
    "    return (\"unused\" in word \n",
    "            or \"#\" in word \n",
    "            or not is_ascii(word)\n",
    "            or len(word) < 3)\n",
    "\n",
    "\n",
    "class BertDataset(Dataset):\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        # get the tokenized words.\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        # load BERT base model\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.bert.eval()\n",
    "\n",
    "        # input characters\n",
    "        self.CHAR_VOCAB_SIZE = 128\n",
    "        words = self.tokenizer.vocab.keys()\n",
    "        self.vocabs = [word for word in words if not is_word_unfeasible(word)]\n",
    "        self.chars = [torch.LongTensor([ord(c) for c in word])\n",
    "                      for word in self.vocabs]\n",
    "        self.chars = rnn.pad_sequence(self.chars).to(self.device).T\n",
    "\n",
    "        # word embeddings of bert\n",
    "        ids = torch.LongTensor(\n",
    "            self.tokenizer.convert_tokens_to_ids(self.vocabs)).to(self.device)\n",
    "        self.word_embed = self.bert.embeddings.word_embeddings(\n",
    "            ids.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocabs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.chars[idx], self.word_embed[idx]\n",
    "\n",
    "\n",
    "class Conv1dBlockBN(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size, stride, p=0.0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channel, out_channel,\n",
    "                      kernel_size=kernel_size, stride=stride),\n",
    "            nn.Dropout(p),\n",
    "            nn.PReLU(),\n",
    "            nn.BatchNorm1d(out_channel)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNN_LM(nn.Module):\n",
    "    def __init__(self, char_vocab_size, char_len, embed_dim, chan_size, hid_size, bert_hid_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(char_vocab_size, embed_dim)\n",
    "        convs = []\n",
    "        for i in range(char_len - 1):\n",
    "            if i == 0:\n",
    "                convs.append(Conv1dBlockBN(embed_dim, chan_size, 2, stride=1))\n",
    "            else:\n",
    "                convs.append(Conv1dBlockBN(chan_size, chan_size, 2, stride=1))\n",
    "        self.convs = nn.Sequential(*convs)\n",
    "        self.fc1 = nn.Linear(chan_size, hid_size)\n",
    "        self.fc2 = nn.Linear(hid_size, bert_hid_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, embed_dim, context_width)\n",
    "        x = self.embedding(x).permute(0, 2, 1)\n",
    "        x = self.convs(x)  # (batch_size, chan_size, 1)\n",
    "        x = x.squeeze(2)  # (batch_size, chan_size)\n",
    "        x = F.relu(self.fc1(x))  # (batch_size, hid_size)\n",
    "        x = self.fc2(x)  # (batch_size, vocab_size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:00<00:00, 256904.14B/s]\n",
      "100%|██████████| 407873900/407873900 [01:05<00:00, 6242065.20B/s]\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (22205 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "dataset = BertDataset(device)\n",
    "dataloader = DataLoader(dataset, batch_size=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_LM(\n",
       "  (embedding): Embedding(128, 16)\n",
       "  (convs): Sequential(\n",
       "    (0): Conv1dBlockBN(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv1d(16, 32, kernel_size=(2,), stride=(1,))\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Conv1dBlockBN(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Conv1dBlockBN(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Conv1dBlockBN(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): Conv1dBlockBN(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Conv1dBlockBN(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Conv1dBlockBN(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Conv1dBlockBN(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): Conv1dBlockBN(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Conv1dBlockBN(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): Conv1dBlockBN(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): Conv1dBlockBN(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): Conv1dBlockBN(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): Conv1dBlockBN(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): Conv1dBlockBN(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): Conv1dBlockBN(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): Conv1dBlockBN(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): PReLU(num_parameters=1)\n",
       "        (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=32, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--num_epochs', type=int, default=100)\n",
    "parser.add_argument('--embed_size', type=int, default=8)\n",
    "parser.add_argument('--hidden_size', type=int, default=256)\n",
    "parser.add_argument('--channel_size', type=int, default=32)\n",
    "parser.add_argument('--batch_size', type=int, default=256)\n",
    "parser.add_argument('--learning_rate', type=float, default=0.001)\n",
    "args = parser.parse_args([\"--num_epochs\", \"10\",\n",
    "                          \"--embed_size\", \"16\", \"--hidden_size\", \"256\",\n",
    "                          \"--batch_size\", \"256\", \n",
    "                          \"--learning_rate\", \"0.001\",\n",
    "                          \"--channel_size\", \"32\"])\n",
    "\n",
    "CHAR_VOCAB_SIZE = 128\n",
    "BERT_EMBED_DIM = 768\n",
    "model = CNN_LM(char_vocab_size=CHAR_VOCAB_SIZE, \n",
    "        char_len=dataset.chars.shape[1], embed_dim=args.embed_size,\n",
    "        chan_size=args.channel_size, hid_size=args.hidden_size,\n",
    "        bert_hid_size=BERT_EMBED_DIM)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(\"data/bert_cnn.ckpt\", map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 18])\n",
      "torch.Size([3, 768])\n"
     ]
    }
   ],
   "source": [
    "for data in dataloader:\n",
    "    _input, _target = data\n",
    "    break\n",
    "print(_input.shape)\n",
    "print(_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]',\n",
       " '[UNK]',\n",
       " '[CLS]',\n",
       " '[SEP]',\n",
       " '[MASK]',\n",
       " 'the',\n",
       " 'and',\n",
       " 'was',\n",
       " 'for',\n",
       " 'with',\n",
       " 'that',\n",
       " 'his',\n",
       " 'from',\n",
       " 'her',\n",
       " 'she',\n",
       " 'you',\n",
       " 'had',\n",
       " 'were',\n",
       " 'but',\n",
       " 'this',\n",
       " 'are',\n",
       " 'not',\n",
       " 'they',\n",
       " 'one',\n",
       " 'which',\n",
       " 'have',\n",
       " 'him',\n",
       " 'first',\n",
       " 'all',\n",
       " 'also',\n",
       " 'their',\n",
       " 'has',\n",
       " 'who',\n",
       " 'out',\n",
       " 'been',\n",
       " 'when',\n",
       " 'after',\n",
       " 'there',\n",
       " 'into',\n",
       " 'new',\n",
       " 'two',\n",
       " 'its',\n",
       " 'time',\n",
       " 'would',\n",
       " 'what',\n",
       " 'about',\n",
       " 'said',\n",
       " 'over',\n",
       " 'then',\n",
       " 'other',\n",
       " 'more',\n",
       " 'can',\n",
       " 'like',\n",
       " 'back',\n",
       " 'them',\n",
       " 'only',\n",
       " 'some',\n",
       " 'could',\n",
       " 'where',\n",
       " 'just',\n",
       " 'during',\n",
       " 'before',\n",
       " 'made',\n",
       " 'school',\n",
       " 'through',\n",
       " 'than',\n",
       " 'now',\n",
       " 'years',\n",
       " 'most',\n",
       " 'world',\n",
       " 'may',\n",
       " 'between',\n",
       " 'down',\n",
       " 'well',\n",
       " 'three',\n",
       " 'year',\n",
       " 'while',\n",
       " 'will',\n",
       " 'later',\n",
       " 'city',\n",
       " 'under',\n",
       " 'around',\n",
       " 'did',\n",
       " 'such',\n",
       " 'being',\n",
       " 'used',\n",
       " 'state',\n",
       " 'people',\n",
       " 'part',\n",
       " 'know',\n",
       " 'against',\n",
       " 'your',\n",
       " 'many',\n",
       " 'second',\n",
       " 'university',\n",
       " 'both',\n",
       " 'national',\n",
       " 'these',\n",
       " 'don',\n",
       " 'known',\n",
       " 'off',\n",
       " 'way',\n",
       " 'until',\n",
       " 'how',\n",
       " 'even',\n",
       " 'get',\n",
       " 'head',\n",
       " '...',\n",
       " 'didn',\n",
       " 'team',\n",
       " 'american',\n",
       " 'because',\n",
       " 'born',\n",
       " 'united',\n",
       " 'film',\n",
       " 'since',\n",
       " 'still',\n",
       " 'long',\n",
       " 'work',\n",
       " 'south',\n",
       " 'became',\n",
       " 'any',\n",
       " 'high',\n",
       " 'again',\n",
       " 'day',\n",
       " 'family',\n",
       " 'see',\n",
       " 'right',\n",
       " 'man',\n",
       " 'eyes',\n",
       " 'house',\n",
       " 'season',\n",
       " 'war',\n",
       " 'states',\n",
       " 'including',\n",
       " 'took',\n",
       " 'life',\n",
       " 'north',\n",
       " 'same',\n",
       " 'each',\n",
       " 'called',\n",
       " 'name',\n",
       " 'much',\n",
       " 'place',\n",
       " 'however',\n",
       " 'four',\n",
       " 'group',\n",
       " 'another',\n",
       " 'found',\n",
       " 'won',\n",
       " 'area',\n",
       " 'here',\n",
       " 'going',\n",
       " 'away',\n",
       " 'series',\n",
       " 'left',\n",
       " 'home',\n",
       " 'music',\n",
       " 'best',\n",
       " 'make',\n",
       " 'hand',\n",
       " 'number',\n",
       " 'company',\n",
       " 'several',\n",
       " 'never',\n",
       " 'last',\n",
       " 'john',\n",
       " '000',\n",
       " 'very',\n",
       " 'album',\n",
       " 'take',\n",
       " 'end',\n",
       " 'good',\n",
       " 'too',\n",
       " 'following',\n",
       " 'released',\n",
       " 'game',\n",
       " 'played',\n",
       " 'little',\n",
       " 'began',\n",
       " 'district',\n",
       " 'old',\n",
       " 'want',\n",
       " 'those',\n",
       " 'side',\n",
       " 'held',\n",
       " 'own',\n",
       " 'early',\n",
       " 'county',\n",
       " 'league',\n",
       " 'use',\n",
       " 'west',\n",
       " 'face',\n",
       " 'think',\n",
       " '2010',\n",
       " 'government',\n",
       " 'march',\n",
       " 'came',\n",
       " 'small',\n",
       " 'general',\n",
       " 'town',\n",
       " 'june',\n",
       " 'line',\n",
       " 'based',\n",
       " 'something',\n",
       " 'september',\n",
       " 'thought',\n",
       " 'looked',\n",
       " 'along',\n",
       " 'international',\n",
       " '2011',\n",
       " 'air',\n",
       " 'july',\n",
       " 'club',\n",
       " 'went',\n",
       " 'january',\n",
       " 'october',\n",
       " 'our',\n",
       " 'august',\n",
       " 'april',\n",
       " 'york',\n",
       " 'few',\n",
       " '2012',\n",
       " '2008',\n",
       " 'east',\n",
       " 'show',\n",
       " 'member',\n",
       " 'college',\n",
       " '2009',\n",
       " 'father',\n",
       " 'public',\n",
       " 'come',\n",
       " 'men',\n",
       " 'five',\n",
       " 'set',\n",
       " 'station',\n",
       " 'church',\n",
       " 'next',\n",
       " 'former',\n",
       " 'november',\n",
       " 'room',\n",
       " 'party',\n",
       " 'located',\n",
       " 'december',\n",
       " '2013',\n",
       " 'age',\n",
       " 'got',\n",
       " '2007',\n",
       " 'system',\n",
       " 'let',\n",
       " 'love',\n",
       " '2006',\n",
       " 'though',\n",
       " 'every',\n",
       " '2014',\n",
       " 'look',\n",
       " 'song',\n",
       " 'water',\n",
       " 'century',\n",
       " 'without',\n",
       " 'body',\n",
       " 'black',\n",
       " 'night',\n",
       " 'within',\n",
       " 'great',\n",
       " 'women',\n",
       " 'single',\n",
       " 'building',\n",
       " 'large',\n",
       " 'population',\n",
       " 'river',\n",
       " 'named',\n",
       " 'band',\n",
       " 'white',\n",
       " 'started',\n",
       " 'once',\n",
       " 'should',\n",
       " '2015',\n",
       " 'service',\n",
       " 'top',\n",
       " 'built',\n",
       " 'british',\n",
       " 'open',\n",
       " 'death',\n",
       " 'king',\n",
       " 'moved',\n",
       " 'local',\n",
       " 'times',\n",
       " 'children',\n",
       " 'february',\n",
       " 'book',\n",
       " 'why',\n",
       " 'door',\n",
       " 'need',\n",
       " 'president',\n",
       " 'order',\n",
       " 'final',\n",
       " 'road',\n",
       " 'wasn',\n",
       " 'although',\n",
       " 'due',\n",
       " 'major',\n",
       " 'died',\n",
       " 'village',\n",
       " 'third',\n",
       " 'knew',\n",
       " '2016',\n",
       " 'asked',\n",
       " 'turned',\n",
       " 'wanted',\n",
       " 'say',\n",
       " 'together',\n",
       " 'received',\n",
       " 'main',\n",
       " 'son',\n",
       " 'served',\n",
       " 'different',\n",
       " 'behind',\n",
       " 'himself',\n",
       " 'felt',\n",
       " 'members',\n",
       " 'power',\n",
       " 'football',\n",
       " 'law',\n",
       " 'voice',\n",
       " 'play',\n",
       " 'near',\n",
       " 'park',\n",
       " 'history',\n",
       " 'having',\n",
       " '2005',\n",
       " 'saw',\n",
       " 'mother',\n",
       " 'army',\n",
       " 'point',\n",
       " 'front',\n",
       " 'help',\n",
       " 'english',\n",
       " 'street',\n",
       " 'art',\n",
       " 'late',\n",
       " 'hands',\n",
       " 'games',\n",
       " 'award',\n",
       " 'young',\n",
       " 'put',\n",
       " 'published',\n",
       " 'country',\n",
       " 'division',\n",
       " 'across',\n",
       " 'told',\n",
       " 'often',\n",
       " 'ever',\n",
       " 'french',\n",
       " 'london',\n",
       " 'center',\n",
       " 'six',\n",
       " 'red',\n",
       " '2017',\n",
       " 'led',\n",
       " 'days',\n",
       " 'include',\n",
       " 'light',\n",
       " 'find',\n",
       " 'tell',\n",
       " 'among',\n",
       " 'species',\n",
       " 'really',\n",
       " 'according',\n",
       " 'central',\n",
       " 'half',\n",
       " '2004',\n",
       " 'form',\n",
       " 'original',\n",
       " 'gave',\n",
       " 'office',\n",
       " 'making',\n",
       " 'enough',\n",
       " 'lost',\n",
       " 'full',\n",
       " 'opened',\n",
       " 'must',\n",
       " 'included',\n",
       " 'live',\n",
       " 'given',\n",
       " 'german',\n",
       " 'player',\n",
       " 'run',\n",
       " 'business',\n",
       " 'woman',\n",
       " 'community',\n",
       " 'cup',\n",
       " 'might',\n",
       " 'million',\n",
       " 'land',\n",
       " '2000',\n",
       " 'court',\n",
       " 'development',\n",
       " 'short',\n",
       " 'round',\n",
       " 'seen',\n",
       " 'class',\n",
       " 'story',\n",
       " 'always',\n",
       " 'become',\n",
       " 'sure',\n",
       " 'research',\n",
       " 'almost',\n",
       " 'director',\n",
       " 'council',\n",
       " 'career',\n",
       " 'things',\n",
       " 'using',\n",
       " 'island',\n",
       " 'couldn',\n",
       " 'car',\n",
       " 'close',\n",
       " 'force',\n",
       " 'better',\n",
       " 'free',\n",
       " 'support',\n",
       " 'control',\n",
       " 'field',\n",
       " 'students',\n",
       " '2003',\n",
       " 'education',\n",
       " 'married',\n",
       " 'nothing',\n",
       " 'worked',\n",
       " 'others',\n",
       " 'record',\n",
       " 'big',\n",
       " 'inside',\n",
       " 'level',\n",
       " 'anything',\n",
       " 'continued',\n",
       " 'give',\n",
       " 'james',\n",
       " 'military',\n",
       " 'established',\n",
       " 'non',\n",
       " 'returned',\n",
       " 'feel',\n",
       " 'does',\n",
       " 'title',\n",
       " 'written',\n",
       " 'thing',\n",
       " 'feet',\n",
       " 'william',\n",
       " 'far',\n",
       " 'association',\n",
       " 'hard',\n",
       " 'already',\n",
       " '2002',\n",
       " 'championship',\n",
       " 'human',\n",
       " 'western',\n",
       " '100',\n",
       " 'department',\n",
       " 'hall',\n",
       " 'role',\n",
       " 'various',\n",
       " 'production',\n",
       " 'heart',\n",
       " '2001',\n",
       " 'living',\n",
       " 'fire',\n",
       " 'version',\n",
       " 'television',\n",
       " 'royal',\n",
       " 'produced',\n",
       " 'working',\n",
       " 'act',\n",
       " 'case',\n",
       " 'society',\n",
       " 'region',\n",
       " 'present',\n",
       " 'radio',\n",
       " 'period',\n",
       " 'looking',\n",
       " 'least',\n",
       " 'total',\n",
       " 'keep',\n",
       " 'england',\n",
       " 'wife',\n",
       " 'program',\n",
       " 'per',\n",
       " 'brother',\n",
       " 'mind',\n",
       " 'special',\n",
       " 'works',\n",
       " 'soon',\n",
       " 'political',\n",
       " 'george',\n",
       " 'services',\n",
       " 'taken',\n",
       " 'created',\n",
       " 'further',\n",
       " 'able',\n",
       " 'reached',\n",
       " 'david',\n",
       " 'union',\n",
       " 'joined',\n",
       " 'upon',\n",
       " 'done',\n",
       " 'important',\n",
       " 'social',\n",
       " 'information',\n",
       " 'either',\n",
       " 'appeared',\n",
       " 'position',\n",
       " 'ground',\n",
       " 'lead',\n",
       " 'rock',\n",
       " 'dark',\n",
       " 'election',\n",
       " 'board',\n",
       " 'france',\n",
       " 'hair',\n",
       " 'course',\n",
       " 'arms',\n",
       " 'site',\n",
       " 'police',\n",
       " 'girl',\n",
       " 'instead',\n",
       " 'real',\n",
       " 'sound',\n",
       " 'words',\n",
       " 'moment',\n",
       " 'someone',\n",
       " 'summer',\n",
       " 'project',\n",
       " 'announced',\n",
       " 'san',\n",
       " 'less',\n",
       " 'wrote',\n",
       " 'past',\n",
       " 'followed',\n",
       " 'blue',\n",
       " 'founded',\n",
       " 'finally',\n",
       " 'india',\n",
       " 'taking',\n",
       " 'records',\n",
       " 'america',\n",
       " '1999',\n",
       " 'design',\n",
       " 'considered',\n",
       " 'northern',\n",
       " 'god',\n",
       " 'stop',\n",
       " 'battle',\n",
       " 'toward',\n",
       " 'european',\n",
       " 'outside',\n",
       " 'described',\n",
       " 'track',\n",
       " 'today',\n",
       " 'playing',\n",
       " 'language',\n",
       " 'call',\n",
       " 'heard',\n",
       " 'professional',\n",
       " 'low',\n",
       " 'australia',\n",
       " 'miles',\n",
       " 'california',\n",
       " 'win',\n",
       " 'yet',\n",
       " 'green',\n",
       " 'trying',\n",
       " 'blood',\n",
       " 'southern',\n",
       " 'science',\n",
       " 'maybe',\n",
       " 'everything',\n",
       " 'match',\n",
       " 'square',\n",
       " 'mouth',\n",
       " 'video',\n",
       " 'race',\n",
       " 'recorded',\n",
       " 'leave',\n",
       " 'above',\n",
       " 'daughter',\n",
       " 'points',\n",
       " 'space',\n",
       " '1998',\n",
       " 'museum',\n",
       " 'change',\n",
       " 'middle',\n",
       " 'common',\n",
       " 'move',\n",
       " 'post',\n",
       " 'lake',\n",
       " 'seven',\n",
       " 'tried',\n",
       " 'elected',\n",
       " 'closed',\n",
       " 'ten',\n",
       " 'paul',\n",
       " 'minister',\n",
       " 'months',\n",
       " 'start',\n",
       " 'chief',\n",
       " 'return',\n",
       " 'canada',\n",
       " 'person',\n",
       " 'sea',\n",
       " 'release',\n",
       " 'similar',\n",
       " 'modern',\n",
       " 'brought',\n",
       " 'rest',\n",
       " 'hit',\n",
       " 'formed',\n",
       " '1997',\n",
       " 'floor',\n",
       " 'event',\n",
       " 'doing',\n",
       " 'thomas',\n",
       " '1996',\n",
       " 'robert',\n",
       " 'care',\n",
       " 'killed',\n",
       " 'training',\n",
       " 'star',\n",
       " 'week',\n",
       " 'needed',\n",
       " 'turn',\n",
       " 'finished',\n",
       " 'railway',\n",
       " 'rather',\n",
       " 'news',\n",
       " 'health',\n",
       " 'sent',\n",
       " 'example',\n",
       " 'ran',\n",
       " 'term',\n",
       " 'michael',\n",
       " 'coming',\n",
       " 'currently',\n",
       " 'yes',\n",
       " 'forces',\n",
       " 'despite',\n",
       " 'gold',\n",
       " 'areas',\n",
       " 'stage',\n",
       " 'fact',\n",
       " 'dead',\n",
       " 'says',\n",
       " 'popular',\n",
       " '2018',\n",
       " 'originally',\n",
       " 'germany',\n",
       " 'probably',\n",
       " 'developed',\n",
       " 'result',\n",
       " 'pulled',\n",
       " 'friend',\n",
       " 'stood',\n",
       " 'money',\n",
       " 'running',\n",
       " 'signed',\n",
       " 'word',\n",
       " 'songs',\n",
       " 'child',\n",
       " 'eventually',\n",
       " 'met',\n",
       " 'tour',\n",
       " 'average',\n",
       " 'teams',\n",
       " 'minutes',\n",
       " 'festival',\n",
       " 'current',\n",
       " 'deep',\n",
       " 'kind',\n",
       " '1995',\n",
       " 'decided',\n",
       " 'usually',\n",
       " 'eastern',\n",
       " 'seemed',\n",
       " 'episode',\n",
       " 'bed',\n",
       " 'added',\n",
       " 'table',\n",
       " 'indian',\n",
       " 'private',\n",
       " 'charles',\n",
       " 'route',\n",
       " 'available',\n",
       " 'idea',\n",
       " 'throughout',\n",
       " 'centre',\n",
       " 'addition',\n",
       " 'appointed',\n",
       " 'style',\n",
       " '1994',\n",
       " 'books',\n",
       " 'eight',\n",
       " 'construction',\n",
       " 'press',\n",
       " 'mean',\n",
       " 'wall',\n",
       " 'friends',\n",
       " 'remained',\n",
       " 'schools',\n",
       " 'study',\n",
       " 'institute',\n",
       " 'chinese',\n",
       " 'sometimes',\n",
       " 'events',\n",
       " 'possible',\n",
       " '1992',\n",
       " 'australian',\n",
       " 'type',\n",
       " 'brown',\n",
       " 'forward',\n",
       " 'talk',\n",
       " 'process',\n",
       " 'food',\n",
       " 'debut',\n",
       " 'seat',\n",
       " 'performance',\n",
       " 'committee',\n",
       " 'features',\n",
       " 'character',\n",
       " 'arts',\n",
       " 'herself',\n",
       " 'else',\n",
       " 'lot',\n",
       " 'strong',\n",
       " 'russian',\n",
       " 'range',\n",
       " 'hours',\n",
       " 'peter',\n",
       " 'arm',\n",
       " 'morning',\n",
       " 'sold',\n",
       " 'quickly',\n",
       " 'directed',\n",
       " '1993',\n",
       " 'guitar',\n",
       " 'china',\n",
       " 'list',\n",
       " 'performed',\n",
       " 'media',\n",
       " 'players',\n",
       " 'smile',\n",
       " 'myself',\n",
       " 'placed',\n",
       " 'coach',\n",
       " 'province',\n",
       " 'towards',\n",
       " 'wouldn',\n",
       " 'leading',\n",
       " 'whole',\n",
       " 'boy',\n",
       " 'official',\n",
       " 'designed',\n",
       " 'grand',\n",
       " 'census',\n",
       " 'europe',\n",
       " 'attack',\n",
       " 'japanese',\n",
       " 'henry',\n",
       " '1991',\n",
       " 'cross',\n",
       " 'getting',\n",
       " 'alone',\n",
       " 'action',\n",
       " 'lower',\n",
       " 'network',\n",
       " 'wide',\n",
       " 'washington',\n",
       " 'japan',\n",
       " '1990',\n",
       " 'hospital',\n",
       " 'believe',\n",
       " 'changed',\n",
       " 'sister',\n",
       " 'hold',\n",
       " 'gone',\n",
       " 'sir',\n",
       " 'hadn',\n",
       " 'ship',\n",
       " 'studies',\n",
       " 'academy',\n",
       " 'shot',\n",
       " 'rights',\n",
       " 'below',\n",
       " 'base',\n",
       " 'bad',\n",
       " 'involved',\n",
       " 'kept',\n",
       " 'largest',\n",
       " 'bank',\n",
       " 'future',\n",
       " 'especially',\n",
       " 'beginning',\n",
       " 'mark',\n",
       " 'movement',\n",
       " 'section',\n",
       " 'female',\n",
       " 'magazine',\n",
       " 'plan',\n",
       " 'professor',\n",
       " 'lord',\n",
       " 'longer',\n",
       " 'sat',\n",
       " 'walked',\n",
       " 'hill',\n",
       " 'actually',\n",
       " 'civil',\n",
       " 'energy',\n",
       " 'model',\n",
       " 'families',\n",
       " 'size',\n",
       " 'thus',\n",
       " 'aircraft',\n",
       " 'completed',\n",
       " 'includes',\n",
       " 'data',\n",
       " 'captain',\n",
       " 'fight',\n",
       " 'vocals',\n",
       " 'featured',\n",
       " 'richard',\n",
       " 'bridge',\n",
       " 'fourth',\n",
       " '1989',\n",
       " 'officer',\n",
       " 'stone',\n",
       " 'hear',\n",
       " 'means',\n",
       " 'medical',\n",
       " 'groups',\n",
       " 'management',\n",
       " 'self',\n",
       " 'lips',\n",
       " 'competition',\n",
       " 'entire',\n",
       " 'lived',\n",
       " 'technology',\n",
       " 'leaving',\n",
       " 'federal',\n",
       " 'tournament',\n",
       " 'bit',\n",
       " 'passed',\n",
       " 'hot',\n",
       " 'independent',\n",
       " 'awards',\n",
       " 'kingdom',\n",
       " 'mary',\n",
       " 'spent',\n",
       " 'fine',\n",
       " 'doesn',\n",
       " 'reported',\n",
       " 'jack',\n",
       " 'fall',\n",
       " 'raised',\n",
       " 'itself',\n",
       " 'stay',\n",
       " 'true',\n",
       " 'studio',\n",
       " '1988',\n",
       " 'sports',\n",
       " 'replaced',\n",
       " 'paris',\n",
       " 'systems',\n",
       " 'saint',\n",
       " 'leader',\n",
       " 'theatre',\n",
       " 'whose',\n",
       " 'market',\n",
       " 'capital',\n",
       " 'parents',\n",
       " 'spanish',\n",
       " 'canadian',\n",
       " 'earth',\n",
       " 'cut',\n",
       " 'degree',\n",
       " 'writing',\n",
       " 'bay',\n",
       " 'christian',\n",
       " 'awarded',\n",
       " 'natural',\n",
       " 'higher',\n",
       " 'bill',\n",
       " 'coast',\n",
       " 'provided',\n",
       " 'previous',\n",
       " 'senior',\n",
       " 'valley',\n",
       " 'organization',\n",
       " 'stopped',\n",
       " 'onto',\n",
       " 'countries',\n",
       " 'parts',\n",
       " 'conference',\n",
       " 'queen',\n",
       " 'security',\n",
       " 'interest',\n",
       " 'saying',\n",
       " 'allowed',\n",
       " 'master',\n",
       " 'earlier',\n",
       " 'phone',\n",
       " 'matter',\n",
       " 'smith',\n",
       " 'winning',\n",
       " 'try',\n",
       " 'happened',\n",
       " 'moving',\n",
       " 'campaign',\n",
       " 'los',\n",
       " 'breath',\n",
       " 'nearly',\n",
       " 'mid',\n",
       " '1987',\n",
       " 'certain',\n",
       " 'girls',\n",
       " 'date',\n",
       " 'italian',\n",
       " 'african',\n",
       " 'standing',\n",
       " 'fell',\n",
       " 'artist',\n",
       " 'shows',\n",
       " 'deal',\n",
       " 'mine',\n",
       " 'industry',\n",
       " '1986',\n",
       " 'everyone',\n",
       " 'republic',\n",
       " 'provide',\n",
       " 'collection',\n",
       " 'library',\n",
       " 'student',\n",
       " 'primary',\n",
       " 'owned',\n",
       " 'older',\n",
       " 'via',\n",
       " 'heavy',\n",
       " '1st',\n",
       " 'makes',\n",
       " 'attention',\n",
       " 'anyone',\n",
       " 'africa',\n",
       " 'stated',\n",
       " 'length',\n",
       " 'ended',\n",
       " 'fingers',\n",
       " 'command',\n",
       " 'staff',\n",
       " 'skin',\n",
       " 'foreign',\n",
       " 'opening',\n",
       " 'governor',\n",
       " 'okay',\n",
       " 'medal',\n",
       " 'kill',\n",
       " 'sun',\n",
       " 'cover',\n",
       " 'job',\n",
       " '1985',\n",
       " 'introduced',\n",
       " 'chest',\n",
       " 'hell',\n",
       " 'feeling',\n",
       " 'success',\n",
       " 'meet',\n",
       " 'reason',\n",
       " 'standard',\n",
       " 'meeting',\n",
       " 'novel',\n",
       " '1984',\n",
       " 'trade',\n",
       " 'source',\n",
       " 'buildings',\n",
       " 'rose',\n",
       " 'guy',\n",
       " 'goal',\n",
       " 'chapter',\n",
       " 'native',\n",
       " 'husband',\n",
       " 'previously',\n",
       " 'unit',\n",
       " 'limited',\n",
       " 'entered',\n",
       " 'weeks',\n",
       " 'producer',\n",
       " 'operations',\n",
       " 'mountain',\n",
       " 'takes',\n",
       " 'covered',\n",
       " 'forced',\n",
       " 'related',\n",
       " 'roman',\n",
       " 'complete',\n",
       " ...]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.vocabs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
